# Exploratory Data Analysis
# The process of reviewing and cleaning data to
# 1- derive insights
# 2- generate hypotheses

import pandas as pd

df = pd.read_csv("Country.csv")
print(df.dtypes)

# if you want to change data type for column 

df["area"] = df["area"].astype("int")
print(df.dtypes)

# if you want to show a specific data type columns: 

print(df.select_dtypes("object") , "\n\n")
print(df.select_dtypes("number"), "\n\n")

print(df["area"].min())
print(df["area"].max())
print("\n\n")
# -------------
x = df.agg({"area": ["mean", "std"], "population": ["median"]})
print(x)

x = df.groupby("country").agg(

    mean_area = ("area"  , "mean"),
    std_area = ("area" , "std"),

)

print(x)


# Addressing missing value 

# 1- Drop missing values 5% or less of total values
# 2- Impute mean, median, mode Depends on distribution and context
# 3- Impute by sub-group:
# Different experience levels have different median salary

# ÙŠØ¹Ù†ÙŠ Ø¥ÙŠÙ‡ Impute by sub-groupØŸ

# Ø¨Ø¯Ù„ Ù…Ø§ ØªÙ…Ù„Ø§ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù†Ø§Ù‚ØµØ©  Ø¨Ù‚ÙŠÙ…Ø© ÙˆØ§Ø­Ø¯Ø© Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¯Ø§ØªØ§ ÙƒÙ„Ù‡Ø§ (Ø²ÙŠ Ø§Ù„Ù…ØªÙˆØ³Ø· )ØŒ
#  Ø¨Ù†Ù‚Ø³Ù… Ø§Ù„Ø¯Ø§ØªØ§ Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù…Ù†Ø·Ù‚ÙŠØ© Ù…Ø«Ù„Ø§Ù‹ Ø­Ø³Ø¨ Ø®Ø¨Ø±Ø© Ø§Ù„Ù…ÙˆØ¸Ù 
# (Junior/Mid/Senior)
# ÙˆØ¨Ø¹Ø¯ÙŠÙ† Ù†Ù…Ù„Ø£ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù†Ø§Ù‚ØµØ© Ø¨Ø¥Ø­ØµØ§Ø¦ÙŠØ© Ù…Ù† Ù†ÙØ³ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© (ØºØ§Ù„Ø¨Ù‹Ø§ Ø§Ù„ÙˆØ³ÙŠØ· Ù„Ø£Ù†Ù‡ Ù…Ù‚Ø§ÙˆÙ… Ù„Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©).
# Ø§Ù„ÙÙƒØ±Ø©: Ø±ÙˆØ§ØªØ¨ Ø§Ù„Ù€ Senior Ø£Ø¹Ù„Ù‰ Ù…Ù† JuniorØŒ ÙÙ„Ùˆ Ù…Ù„ÙŠØª NaN Ø¨ÙˆØ³ÙŠØ· ÙƒÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù‡ØªØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„ÙØ±ÙˆÙ‚Ø§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨ÙŠÙ† Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª.

import pandas as pd
df = pd.read_csv("employee_salaries_original.csv")

# print number of nan value 

print(df.isna().sum())

# 1- first method drop less 5%

threshold = len(df) * 0.05

cols_drop = df.columns[(df.isna().sum() <= threshold ) & (df.isna().sum() != 0)]

df.dropna(subset=cols_drop , inplace= True)
print(df.isna().sum())

# Method 2 

cols_with_missing_values = df.columns[df.isna().sum() > 0 ]

for col in cols_with_missing_values: 
    df[col].fillna(df[col].mode()[0] , inplace= True)

print(df.isna().sum())

# method 3 

import pandas as pd
df = pd.read_csv("employee_salaries_original.csv")

salaries_dict = df.groupby("experience_level")["salary"].median().to_dict()
print(salaries_dict)

df["salary"] = df["salary"].fillna(df["experience_level"].map(salaries_dict))
print(df.isna().sum())

# -----------------------

# Converting and analyzing categorical data

import pandas as pd
df = pd.read_csv("employee_salaries_original.csv")

print(df["experience_level"].nunique(),"\n\n")

check = df["experience_level"].str.contains("ior")
print(df[check],"\n\n")

# Senior or junior
check = df["experience_level"].str.contains("Senior|Junior") 
print(df[check],"\n\n")

# start with :

check = df["experience_level"].str.contains("^S")
print(df[check],"\n\n")

# --------------

# 1. Ø¹Ù†Ø¯Ùƒ Ù…Ø³Ù…ÙŠØ§Øª ÙˆØ¸ÙŠÙÙŠØ© ÙƒØªÙŠØ±Ø© ÙˆÙ…Ø®ØªÙ„ÙØ©

# Ù…Ø«Ù„Ø§Ù‹ ÙÙŠ Ø¹Ù…ÙˆØ¯ Designation Ù…Ù…ÙƒÙ† ØªÙ„Ø§Ù‚ÙŠ:

# "Senior Data Scientist"

# "NLP Engineer"

# "Data Analyst Intern"

# "ETL Architect"

# "AI Researcher"

# "Lead Data Engineer"

# "Freelance Consultant"
# ÙˆÙ‡ÙƒØ°Ø§â€¦

# Ù„Ùˆ ÙØ¶Ù„Øª ØªØ´ØªØºÙ„ Ø¨Ø§Ù„Ù…Ø³Ù…ÙŠØ§Øª Ø¯ÙŠ Ù‡ØªÙ„Ø§Ù‚ÙŠ Ø¹Ù†Ø¯Ùƒ Ø¹Ø´Ø±Ø§Øª Ø£Ùˆ Ù…Ø¦Ø§Øª Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©.

# 2. Ø§Ù„Ù‡Ø¯Ù â†’ ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø³Ù…ÙŠØ§Øª ØªØ­Øª ÙØ¦Ø§Øª ÙƒØ¨Ø±Ù‰

# ÙØ¹Ù…Ù„Øª Ù„Ø³ØªØ© Ø¨Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©:

# job_categories = ["Data Science", "Data Analytics", "Data Engineering", 
#                   "Machine Learning", "Managerial", "Consultant"] 

# 3. ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù€ Regex Patterns Ù„ÙƒÙ„ ÙØ¦Ø©

# ÙƒØªØ¨Øª Strings ÙÙŠÙ‡Ø§ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© (regular expressions):

# data_science = "Data Scientist|NLP"
# data_analyst = "Analyst|Analytics"
# data_engineer = "Data Engineer|ETL|Architect|Infrastructure"
# ml_engineer = "Machine Learning|ML|Big Data|AI"
# manager = "Manager|Head|Director|Lead|Principal|Staff"
# consultant = "Consultant|Freelance"


# Ø§Ù„Ù…Ø¹Ù†Ù‰: Ù„Ùˆ Ø§Ù„Ù€ Designation ÙŠØ­ØªÙˆÙŠ Ø£ÙŠ ÙƒÙ„Ù…Ø© Ù…Ù† Ø¯ÙˆÙ„ â†’ ÙŠØ¨Ù‚Ù‰ ÙŠÙ†ØªÙ…ÙŠ Ù„Ù„ÙØ¦Ø© Ø¯ÙŠ.
# Ù…Ø«Ù„Ø§Ù‹: "Senior NLP Engineer" Ù‡ÙŠÙ…Ø³Ùƒ ÙƒÙ„Ù…Ø© NLP â†’ ÙŠØªØµÙ†Ù ÙƒÙ€ Data Science.

# 4. conditions
# conditions = [
#     (salaries["Designation"].str.contains(data_science)),
#     (salaries["Designation"].str.contains(data_analyst)),
#     (salaries["Designation"].str.contains(data_engineer)),
#     (salaries["Designation"].str.contains(ml_engineer)),
#     (salaries["Designation"].str.contains(manager)),
#     (salaries["Designation"].str.contains(consultant))
# ]


# Ø¯ÙŠ Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù† Ù‚Ø§Ø¦Ù…Ø© boolean masks (True/False) Ø¹Ù„Ù‰ Ø­Ø³Ø¨ ÙˆØ¬ÙˆØ¯ Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© ÙÙŠ Ø§Ù„Ø¹Ù…ÙˆØ¯.

# 5. np.select
# salaries["Job_Category"] = np.select(
#     conditions,
#     job_categories,
#     default="Other"
# )


# np.select Ø¨ÙŠØ§Ø®Ø¯ Ø§Ù„Ù€ conditions ÙˆÙŠÙ‚Ø§Ø±Ù†Ù‡Ø§ Ø¨Ø§Ù„Ù€ job_categories Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨.

# Ø£ÙˆÙ„ condition ØªØ·Ù„Ø¹ True â†’ ÙŠØ¯ÙŠ Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ù…Ù†Ø§Ø¸Ø±Ø© Ù…Ù† job_categories.

# Ù„Ùˆ Ù…ÙÙŠØ´ ÙˆÙ„Ø§ ÙˆØ§Ø­Ø¯Ø© ØªÙ†Ø·Ø¨Ù‚ â†’ ÙŠØµÙ†ÙÙ‡Ø§ "Other".

# -----------------------

# Working with numeric data

# pd.Series.str.replace("characters to remove", "characters to replace them with")

#salaries["Salary_In_Rupees"] = salaries["Salary_In_Rupees"].str.replace(",", "")

import pandas as pd 

salaries = {"Salary_In_Rupees": ["12,12,3" , "31,41,2" , "5,31,3"] , 
            "Salary_USD" : [12,123,32] ,
            "Experience": ["a" ,"a" , "b"]}

df = pd.DataFrame(salaries)

print(df)

df["Salary_In_Rupees"] = df["Salary_In_Rupees"].str.replace(",","")
print(df)
 
print(df["Salary_In_Rupees"].dtypes) # Object 

df["Salary_In_Rupees"] = df["Salary_In_Rupees"].astype("float")
print(df["Salary_In_Rupees"].dtypes) # float 

df["std_dev"] = df.groupby("Experience")["Salary_USD"].transform(lambda x : x.std())
print(df)

# df["std_dev"] = df.groupby("Experience")["Salary_USD"].transform(lambda x: x.std())
# Ù‡Ù†Ø§ transform Ø¨ØªØ±Ø¬Ø¹ Series Ø¨Ù†ÙØ³ Ø·ÙˆÙ„ DataFrame Ø§Ù„Ø£ØµÙ„ÙŠ.

# Ù„ÙƒÙ„ ØµÙØŒ Ø¨ØªÙƒØªØ¨Ù„Ù‡ Ø§Ù„Ù€ std Ø§Ù„Ø®Ø§Øµ Ø¨Ø§Ù„Ø¬Ø±ÙˆØ¨ Ø§Ù„Ù„ÙŠ Ø¨ÙŠÙ†ØªÙ…ÙŠ Ù„Ù‡.

# ÙŠØ¹Ù†ÙŠ: Ù„Ùˆ Ø¹Ù†Ø¯Ùƒ 100 ØµÙØŒ Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ù‡ÙŠÙƒÙˆÙ† Ø¨Ø±Ø¶Ù‡ 100 ØµÙ.


# ------------------------------------

# Handling outliers 
# Ù„Ùˆ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠÙ‡ ÙØ§ ØºØ§Ù„Ø¨Ø§ Ù‡Ù†Ø³ÙŠØ¨Ù‡Ø§ Ø­ØªÙŠ Ù„Ùˆ ÙÙŠ outlier
# Ù„Ùˆ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø´ÙƒÙˆÙƒ ÙÙŠ ØµØ­ØªÙ‡Ø§ , Ù‡Ù†Ø´ÙŠÙ„Ù‡Ø§ 

# Patterns over time

import pandas as pd

# To convert object type into date type
df= pd.read_csv("divorce.csv", parse_dates=["marriage_date"])
print(df["marriage_date"].dtype)

# OR

df= pd.read_csv("divorce.csv")

df["marriage_date"] = pd.to_datetime(df["marriage_date"])
print(df["marriage_date"].dtype)


import pandas as pd

df= pd.read_csv("divorce.csv")

# Creating datatime data 

df["marriage_date"] = pd.to_datetime(df["marriage_date"])

df["month"] = df["marriage_date"].dt.month
df["day"] = df["marriage_date"].dt.day
df["year"] = df["marriage_date"].dt.year
print(df)


# - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# Correlation
# Describes direction and strength of relationship between two variables

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
df= pd.read_csv("divorce.csv")
cor = df.corr(numeric_only=True)
print(cor)

sns.heatmap(cor , annot=True)  # , annot=True -> set the number in the heatmap 
plt.show()

# relation between this columns in scatter plot
sns.pairplot(data=df, vars=["income_man", "income_woman", "marriage_duration"])
plt.show()


#1. Scatter Plots Ø¨ÙŠÙ† ÙƒÙ„ Ø¹Ù…ÙˆØ¯ÙŠÙ†

# ÙŠØ±Ø³Ù… Ù„Ùƒ scatter plot Ù„ÙƒÙ„ Ø²ÙˆØ¬ Ù…Ù† Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù„ÙŠ Ø§Ø®ØªØ±ØªÙ‡Ù….
# Ù…Ø«Ù„Ø§Ù‹:

# Ù…Ø­ÙˆØ± X = income_manØŒ Ù…Ø­ÙˆØ± Y = income_woman

# Ù…Ø­ÙˆØ± X = income_manØŒ Ù…Ø­ÙˆØ± Y = marriage_duration

# Ù…Ø­ÙˆØ± X = income_womanØŒ Ù…Ø­ÙˆØ± Y = marriage_duration

# ÙˆØ¨ÙƒØ¯Ù‡ ØªÙ‚Ø¯Ø± ØªØ´ÙˆÙ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ø¨ÙŠÙ† ÙƒÙ„ Ø§ØªÙ†ÙŠÙ†.

# 2. ØªÙˆØ²ÙŠØ¹Ø§Øª Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ù†ÙØ³Ù‡Ø§ (diagonal)

# Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø·Ø± (diagonal) Ø¨ÙŠØ¹Ø±Ø¶ histogram Ø£Ùˆ KDE (distribution) Ù„ÙƒÙ„ Ø¹Ù…ÙˆØ¯ Ù„ÙˆØ­Ø¯Ù‡ØŒ Ø¹Ø´Ø§Ù† ØªØ¹Ø±Ù Ø´ÙƒÙ„ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.

# 3. Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ

# ÙŠØ³Ø§Ø¹Ø¯Ùƒ ØªÙƒØªØ´Ù Ø¨Ø³Ø±Ø¹Ø© Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª ÙˆØ§Ù„Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª Ø¨ÙŠÙ† Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª.

# ØªØ´ÙˆÙ Ø¥Ø°Ø§ ÙÙŠÙ‡ Ø¹Ù„Ø§Ù‚Ø© Ø®Ø·ÙŠØ© (linear)ØŒ Ø£Ùˆ ØºÙŠØ± Ø®Ø·ÙŠØ©ØŒ Ø£Ùˆ Ù…ÙÙŠØ´ Ø¹Ù„Ø§Ù‚Ø©.

# ØªØ´ÙˆÙ Ø§Ù„Ù€ outliers (Ù‚ÙŠÙ… Ø´Ø§Ø°Ø©).



# ------------

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
df= pd.read_csv("divorce.csv")
sns.kdeplot(data=df, x="marriage_duration", hue="education_man")
plt.show()


import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
df= pd.read_csv("divorce.csv")
sns.kdeplot(data=df, x="marriage_duration", hue="education_man" , cut = 0)
plt.show()


import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
df= pd.read_csv("divorce.csv")
sns.kdeplot(data=df, x="marriage_duration", hue="education_man" , cut = 0 , cumulative=True)
plt.show()

# --------------------

# Why perform EDA?
# Detecting patterns and relationships
# Generating questions, or hypotheses
# Preparing data for machine learning




# pd.crosstab(
#     planes["Source"], 
#     planes["Destination"], 
#     values=planes["Price"], 
#     aggfunc="median"
# )
# 1. pd.crosstab
# Ø¨ØªØ¹Ù…Ù„ Ø¬Ø¯ÙˆÙ„ Ù…Ù„Ø®Øµ (pivot-style table).

# Ø§Ù„ØµÙÙˆÙ = Ø§Ù„Ù‚ÙŠÙ… ÙÙŠ Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø£ÙˆÙ„ (Source)

# Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© = Ø§Ù„Ù‚ÙŠÙ… ÙÙŠ Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø«Ø§Ù†ÙŠ (Destination)

# 2. values=planes["Price"]
# Ø¨Ø¯Ù„ Ù…Ø§ ØªØ­Ø³Ø¨ Ø¹Ø¯Ù‘ (count) Ø²ÙŠ Ù…Ø§ Ø§Ù„Ù€ crosstab Ø¨ÙŠØ¹Ù…Ù„ Ø¹Ø§Ø¯Ø©ØŒ Ù‡Ù†Ø§ Ø¨ØªØ§Ø®Ø¯ Ø¹Ù…ÙˆØ¯ "Price" ÙˆØªØ·Ø¨Ù‘Ù‚ Ø¹Ù„ÙŠÙ‡ Ø¯Ø§Ù„Ø© ØªØ¬Ù…ÙŠØ¹ (aggregation).

# 3. aggfunc="median"
# Ø¨Ø¯Ù„ Ø§Ù„Ù€ sum Ø£Ùˆ meanØŒ Ù‡Ù†Ø§ Ø¨ØªØ§Ø®Ø¯ Ø§Ù„ÙˆØ³ÙŠØ· (median) Ù„Ø³Ø¹Ø± Ø§Ù„Ø±Ø­Ù„Ø© (Price) Ø¨ÙŠÙ† ÙƒÙ„ Ø²ÙˆØ¬ (Source, Destination).

# 4. Ø§Ù„Ù†ØªÙŠØ¬Ø©:
# Ø¬Ø¯ÙˆÙ„ Ø«Ù†Ø§Ø¦ÙŠ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯:

# Ø§Ù„Ù€ ØµÙÙˆÙ = Ù…Ø¯Ù† Ø§Ù„Ù…ØºØ§Ø¯Ø±Ø© (Source).

# Ø§Ù„Ù€ Ø£Ø¹Ù…Ø¯Ø© = Ù…Ø¯Ù† Ø§Ù„ÙˆØµÙˆÙ„ (Destination).

# Ø§Ù„Ù€ Ù‚ÙŠÙ… Ø§Ù„Ø¬Ø¯ÙˆÙ„ = Ø§Ù„Ù€ median Ù„Ø£Ø³Ø¹Ø§Ø± Ø§Ù„ØªØ°Ø§ÙƒØ± Ù…Ù† Ø§Ù„Ù…ØµØ¯Ø± Ø¥Ù„Ù‰ Ø§Ù„ÙˆØ¬Ù‡Ø©.

# ğŸ“Š Ù…Ø«Ø§Ù„ (Ø§ÙØªØ±Ø§Ø¶ÙŠ):

# Source	Delhi	Mumbai	Chennai
# Kolkata	4500	5200	6100
# Delhi 	NaN	    4000	4800
# Mumbai	4200	NaN	    5000

# Ø¯Ù‡ Ù…Ø¹Ù†Ø§Ù‡ Ù…Ø«Ù„Ø§Ù‹ Ø¥Ù†:

# median Ø³Ø¹Ø± Ø§Ù„Ø±Ø­Ù„Ø© Ù…Ù† Kolkata â†’ Mumbai Ù‡Ùˆ 5200.

# Ù…Ù† Delhi â†’ Chennai Ù‡Ùˆ 4800.

# Ù„Ùˆ Ù…ÙÙŠØ´ Ø±Ø­Ù„Ø© Ø¨ÙŠÙ† Ù…Ø¯ÙŠÙ†ØªÙŠÙ† â†’ ØªØ·Ù„Ø¹ NaN.


# --------------------------

import pandas as pd

planes = pd.DataFrame({
    "Price": [100, 200, 300, 400, 500, 600, 1000, 2000]
})


twenty_fifth = planes["Price"].quantile(0.25)
median = planes["Price"].median()
seventy_fifth = planes["Price"].quantile(0.75)
maximum = planes["Price"].max()

labels = ["Economy", "Premium Economy", "Business Class", "First Class"]
bins = [0, twenty_fifth, median, seventy_fifth, maximum]

planes["Price_Category"] = pd.cut(planes["Price"],labels=labels, bins=bins)
print(planes)

#Ø§Ù„Ù†ØªÙŠØ¬Ø©: Ø§Ù„ÙƒÙˆØ¯ Ø¨ÙŠÙ‚Ø³Ù… Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ù„Ù…Ø³ØªÙˆÙŠØ§Øª (Economy â†’ First Class) Ø­Ø³Ø¨ ØªÙˆØ²ÙŠØ¹Ù‡Ø§ (quartiles + max).

# -------------------------

# Generating hypotheses
